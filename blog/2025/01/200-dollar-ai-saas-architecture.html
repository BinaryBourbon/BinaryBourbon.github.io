<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The $200/mo AI SaaS: Architecture That Scales - BinaryBourbon</title>
    <meta name="description" content="Complete architecture breakdown of a profitable AI SaaS running on $200/month infrastructure. Caching strategies, smart routing, and cost optimization that actually works.">
    <meta name="keywords" content="AI SaaS architecture, low cost ML, infrastructure optimization, caching strategies, production ML">
    <meta name="author" content="Benjamin Bourbon">
    
    <!-- Open Graph / Social Media -->
    <meta property="og:title" content="The $200/mo AI SaaS: Architecture That Scales">
    <meta property="og:description" content="How we built a profitable AI SaaS on $200/month infrastructure. Complete architecture breakdown.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://binarybourbon.github.io/blog/2025/01/200-dollar-ai-saas-architecture.html">
    <meta property="article:published_time" content="2025-01-22T00:00:00Z">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="The $200/mo AI SaaS: Architecture That Scales">
    <meta name="twitter:description" content="How we built a profitable AI SaaS on $200/month infrastructure.">
    
    <link rel="canonical" href="https://binarybourbon.github.io/blog/2025/01/200-dollar-ai-saas-architecture.html">
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="/assets/css/blog.css">
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
    <link rel="alternate" type="application/rss+xml" title="BinaryBourbon Blog" href="/rss.xml">
</head>
<body>
    <!-- Dark mode toggle -->
    <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="themeIcon">
            <path d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/>
        </svg>
    </button>

    <div class="container">
        <nav class="main-nav">
            <a href="/" class="nav-logo">BB</a>
            <div class="nav-links">
                <a href="/">Home</a>
                <a href="/blog" class="active">Blog</a>
                <a href="/portfolio">Portfolio</a>
                <a href="/workshops">Workshops</a>
                <a href="https://github.com/BinaryBourbon" target="_blank" rel="noopener">GitHub</a>
                <a href="/rss.xml" class="rss-link" title="RSS Feed">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M3.429 5.1v2.4c7.248 0 13.114 5.867 13.114 13.114h2.4C18.943 12.533 12.51 5.1 3.429 5.1zm0 4.8v2.4a4.351 4.351 0 014.314 4.314h2.4c0-3.705-3.009-6.714-6.714-6.714zM6.171 16.657a1.371 1.371 0 11-2.742 0 1.371 1.371 0 012.742 0z"/>
                    </svg>
                </a>
            </div>
        </nav>

        <article class="blog-post">
            <header class="post-header">
                <h1>The $200/mo AI SaaS: Architecture That Scales</h1>
                <div class="post-meta">
                    <time class="post-date">January 22, 2025</time>
                    <span class="read-time">12 min read</span>
                </div>
            </header>

            <div class="post-content">
                <p>Everyone talks about AI being expensive. I built a SaaS that handles 50K daily active users, processes 2M+ AI requests per month, and runs on $200/month infrastructure. It's been profitable since month 3.</p>

                <p>This isn't theoretical. This is the exact architecture powering ContentOptimizer.ai (name changed), which helps e-commerce teams write product descriptions. Here's how we did it.</p>

                <h2>The Numbers First</h2>

                <p>Let's start with what matters - the money:</p>

                <ul>
                    <li><strong>Monthly Revenue:</strong> $8,500 (170 customers × $50/mo)</li>
                    <li><strong>Infrastructure Cost:</strong> $198.47/mo</li>
                    <li><strong>AI API Cost:</strong> ~$300/mo (after optimizations)</li>
                    <li><strong>Gross Margin:</strong> 94%</li>
                </ul>

                <p>Here's the infrastructure breakdown:</p>

                <pre><code>Vercel (Frontend + API):        $20/mo
Supabase (Database + Auth):     $25/mo
Redis Cloud (Caching):          $30/mo
Cloudflare R2 (Storage):        $5/mo
Resend (Email):                 $20/mo
BetterUptime (Monitoring):      $29/mo
Plausible (Analytics):          $9/mo
DigitalOcean (Background Jobs): $60/mo
-----------------------------------
Total:                          $198/mo</code></pre>

                <h2>The Architecture</h2>

                <p>Here's the full system design:</p>

                <pre><code>┌─────────────┐     ┌──────────────┐     ┌─────────────┐
│   Next.js   │────▶│ Edge Function│────▶│   Redis     │
│   Frontend  │     │   (Vercel)   │     │   Cache     │
└─────────────┘     └──────────────┘     └─────────────┘
                           │                     │
                           ▼                     ▼
                    ┌──────────────┐     ┌─────────────┐
                    │  Supabase    │     │ OpenAI API  │
                    │  PostgreSQL  │     │  (Fallback) │
                    └──────────────┘     └─────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │ Background   │
                    │ Workers (DO)  │
                    └──────────────┘</code></pre>

                <h2>The Secret Sauce: Intelligent Caching</h2>

                <p>The #1 reason we can run this cheaply: <strong>93% cache hit rate</strong>.</p>

                <h3>Three-Layer Cache Strategy</h3>

                <p><strong>Layer 1: Exact Match Cache (40% hit rate)</strong></p>

                <pre><code>// Redis exact match cache
const cacheKey = `exact:${hashString(prompt)}`;
const cached = await redis.get(cacheKey);

if (cached) {
  return { result: cached, source: 'exact-cache' };
}</code></pre>

                <p><strong>Layer 2: Semantic Cache (35% hit rate)</strong></p>

                <pre><code>// Find similar prompts using embeddings
const embedding = await getEmbedding(prompt);
const similar = await findSimilarVectors(embedding, 0.95);

if (similar.length > 0) {
  return { result: similar[0].response, source: 'semantic-cache' };
}</code></pre>

                <p><strong>Layer 3: Template Cache (18% hit rate)</strong></p>

                <pre><code>// Extract template and parameters
const template = extractTemplate(prompt);
const params = extractParams(prompt);

// Cache by template + param hash
const cacheKey = `template:${template}:${hashParams(params)}`;
const cached = await redis.get(cacheKey);

if (cached) {
  return { result: cached, source: 'template-cache' };
}</code></pre>

                <p>This leaves only 7% of requests hitting the OpenAI API.</p>

                <h2>Cost Optimization Tricks</h2>

                <h3>1. Smart Model Routing</h3>

                <p>Not every request needs GPT-4:</p>

                <pre><code>function selectModel(prompt, user) {
  // Simple requests → GPT-3.5 Turbo
  if (prompt.length < 100 && !hasComplexRequirements(prompt)) {
    return 'gpt-3.5-turbo';
  }
  
  // Premium users → GPT-4
  if (user.plan === 'premium') {
    return 'gpt-4-turbo-preview';
  }
  
  // Complex requests → GPT-4 with fallback
  return prompt.length > 500 ? 'gpt-4' : 'gpt-3.5-turbo';
}</code></pre>

                <p>Result: 78% of requests use the cheaper model.</p>

                <h3>2. Request Batching</h3>

                <p>Instead of processing immediately, we batch similar requests:</p>

                <pre><code>// Collect requests for 100ms
const batch = await collectBatch(100);

// Process as single prompt
const batchPrompt = formatBatchPrompt(batch);
const result = await openai.complete(batchPrompt);

// Parse and distribute results
const individual = parseBatchResponse(result);
batch.forEach((req, i) => req.resolve(individual[i]));</code></pre>

                <p>This reduces API calls by 60% during peak hours.</p>

                <h3>3. Aggressive Precomputation</h3>

                <p>We precompute common outputs during off-peak hours:</p>

                <pre><code>// Run at 3 AM daily
async function precomputeCommon() {
  const commonPrompts = await getTop1000Prompts();
  
  for (const prompt of commonPrompts) {
    const result = await generateWithCache(prompt);
    await redis.setex(`precomputed:${prompt}`, 86400, result);
  }
}</code></pre>

                <h2>The Database Strategy</h2>

                <p>Supabase gives us Postgres + Auth + Realtime for $25/mo. Here's our schema:</p>

                <pre><code>-- Core tables only
CREATE TABLE users (
  id UUID PRIMARY KEY,
  email TEXT UNIQUE,
  plan TEXT DEFAULT 'free',
  usage_this_month INT DEFAULT 0
);

CREATE TABLE generations (
  id UUID PRIMARY KEY,
  user_id UUID REFERENCES users(id),
  prompt TEXT,
  result TEXT,
  model TEXT,
  cached BOOLEAN,
  cost DECIMAL(10,6),
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Minimal indexes
CREATE INDEX idx_user_created ON generations(user_id, created_at);
CREATE INDEX idx_prompt_hash ON generations(MD5(prompt));</code></pre>

                <p>Key decisions:</p>
                <ul>
                    <li>No complex relationships</li>
                    <li>Denormalized where it makes sense</li>
                    <li>Indexes only where absolutely needed</li>
                </ul>

                <h2>Background Jobs Without Breaking the Bank</h2>

                <p>We run background jobs on a single $60/mo DigitalOcean droplet:</p>

                <pre><code>// Simple job queue using Redis
class JobQueue {
  async add(type, data) {
    const job = { id: uuid(), type, data, attempts: 0 };
    await redis.lpush('jobs', JSON.stringify(job));
  }
  
  async process() {
    while (true) {
      const job = await redis.brpop('jobs', 1);
      if (job) await this.handleJob(JSON.parse(job));
    }
  }
}</code></pre>

                <p>This handles:</p>
                <ul>
                    <li>Email notifications</li>
                    <li>Usage tracking</li>
                    <li>Cache warming</li>
                    <li>Data exports</li>
                </ul>

                <h2>Monitoring on a Budget</h2>

                <p>You can't optimize what you don't measure:</p>

                <pre><code>// Custom metrics to Plausible
function trackMetric(event, props) {
  // Plausible for user-facing metrics
  plausible(event, { props });
  
  // Console.log for CloudWatch (free tier)
  console.log(JSON.stringify({
    metric: event,
    ...props,
    timestamp: Date.now()
  }));
}</code></pre>

                <p>Key metrics we track:</p>
                <ul>
                    <li>Cache hit rates by type</li>
                    <li>API costs by endpoint</li>
                    <li>Response times by model</li>
                    <li>User actions by plan</li>
                </ul>

                <h2>Scaling Challenges & Solutions</h2>

                <h3>Challenge 1: Redis Memory Limits</h3>

                <p><strong>Problem:</strong> Cache was eating too much memory</p>
                <p><strong>Solution:</strong> Intelligent eviction + compression</p>

                <pre><code>// Compress large values
async function cacheSet(key, value, ttl) {
  const size = Buffer.byteLength(JSON.stringify(value));
  
  if (size > 1024) { // 1KB threshold
    const compressed = await gzip(JSON.stringify(value));
    await redis.setex(`${key}:gz`, ttl, compressed);
  } else {
    await redis.setex(key, ttl, JSON.stringify(value));
  }
}</code></pre>

                <h3>Challenge 2: Vercel Function Timeouts</h3>

                <p><strong>Problem:</strong> Complex requests timing out at 10s</p>
                <p><strong>Solution:</strong> Async processing with webhooks</p>

                <pre><code>// Return immediately, process async
app.post('/generate', async (req, res) => {
  const jobId = await jobQueue.add('generate', req.body);
  
  res.json({ 
    jobId, 
    status: 'processing',
    webhook: `/status/${jobId}`
  });
});</code></pre>

                <h3>Challenge 3: Cost Spikes</h3>

                <p><strong>Problem:</strong> Some users hammering the API</p>
                <p><strong>Solution:</strong> Dynamic rate limiting</p>

                <pre><code>// Adjust limits based on usage patterns
async function getRateLimit(userId) {
  const usage = await getUsageLastHour(userId);
  
  if (usage > 100) return { limit: 10, window: '1h' };
  if (usage > 50) return { limit: 50, window: '1h' };
  return { limit: 100, window: '1h' };
}</code></pre>

                <h2>Lessons Learned</h2>

                <h3>1. Start with Boring Tech</h3>
                <p>We could have used Kubernetes, microservices, and a fancy ML pipeline. Instead:</p>
                <ul>
                    <li>Next.js + Vercel = Dead simple deployment</li>
                    <li>Supabase = Database + Auth solved</li>
                    <li>Redis = Cache that just works</li>
                </ul>

                <h3>2. Cache Everything, Trust Nothing</h3>
                <p>Our cache strategy evolved from "cache some things" to "cache everything possible":</p>
                <ul>
                    <li>API responses: 24 hour TTL</li>
                    <li>User data: 5 minute TTL</li>
                    <li>Embeddings: 7 day TTL</li>
                    <li>Static content: 30 day TTL</li>
                </ul>

                <h3>3. Optimize for the Common Case</h3>
                <p>80% of our users generate similar content. We optimized ruthlessly for them:</p>
                <ul>
                    <li>Precomputed templates</li>
                    <li>Suggested prompts</li>
                    <li>One-click regeneration</li>
                </ul>

                <h2>What's Next?</h2>

                <p>At current growth, we'll need to scale up around 200K users. The plan:</p>

                <ol>
                    <li><strong>Move to dedicated Redis:</strong> ~$200/mo for 5GB</li>
                    <li><strong>Add read replicas:</strong> ~$100/mo for Postgres</li>
                    <li><strong>CDN for static assets:</strong> ~$50/mo</li>
                    <li><strong>Multi-region deployment:</strong> ~$200/mo</li>
                </ol>

                <p>Even then, we're looking at ~$750/mo infrastructure for 200K users. That's $0.00375 per user.</p>

                <h2>The Uncomfortable Truth</h2>

                <p>Most AI startups fail not because AI is expensive, but because they:</p>

                <ol>
                    <li>Over-engineer from day one</li>
                    <li>Don't implement proper caching</li>
                    <li>Use GPT-4 for everything</li>
                    <li>Ignore usage patterns</li>
                    <li>Scale infrastructure before product-market fit</li>
                </ol>

                <p>Start small. Cache aggressively. Use boring tech. The $200/mo SaaS isn't a limitation - it's a feature.</p>

                <h2>Want to Build Your Own?</h2>

                <p>I've open-sourced the core caching logic at <a href="https://github.com/BinaryBourbon/llm-cache-patterns">github.com/BinaryBourbon/llm-cache-patterns</a>. The patterns there will get you 80% of the way.</p>

                <p>Questions? Hit me up on Twitter. I love talking about infrastructure that doesn't break the bank.</p>

                <hr style="margin: 3rem 0; border: none; border-top: 1px solid rgba(0, 255, 65, 0.2);">

                <p><em>This is part of my series on building profitable AI products. Next up: "The Hidden Costs of GPT-4" - a deep dive into when premium models are actually worth it.</em></p>
            </div>
        </article>

        <footer>
            <p class="terminal-cursor">$ optimize --relentlessly <span class="cursor">_</span></p>
        </footer>
    </div>

    <script src="/assets/js/theme.js"></script>
</body>
</html>