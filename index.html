<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BinaryBourbon - Production ML Systems</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <h1 class="glitch" data-text="BinaryBourbon">BinaryBourbon</h1>
            <p class="tagline">Senior Staff Engineer | Production ML | Making Models Actually Work</p>
        </header>

        <section class="bio">
            <h2>// About</h2>
            <p>12+ years shipping ML at scale. Former Amazon and Stripe. Now helping companies 
            move models from Jupyter notebooks to production systems that don't page you at 3 AM.</p>
            
            <div class="stats">
                <div class="stat">
                    <span class="number">73%</span>
                    <span class="label">Inference Latency Reduced</span>
                </div>
                <div class="stat">
                    <span class="number">$200/mo</span>
                    <span class="label">AI SaaS Infrastructure Cost</span>
                </div>
                <div class="stat">
                    <span class="number">3</span>
                    <span class="label">ML Patents Held</span>
                </div>
            </div>
        </section>

        <section class="facts">
            <h2>// Hard-Won Production Insights</h2>
            <ul class="fact-list">
                <li>Most AI problems are actually data pipeline problems - 80% of production failures come from data drift, not model performance</li>
                <li>A well-tuned BERT model often beats GPT-4 for specific tasks at 1/1000th the cost - know when to use a sledgehammer vs a scalpel</li>
                <li>Caching embeddings properly can reduce inference costs by 95% - most production requests see repeated inputs</li>
                <li>The difference between a PoC and production ML is error handling - your model will see inputs you never imagined</li>
                <li>Quantization isn't just about memory - int8 inference can be 4x faster on modern hardware with <2% accuracy loss</li>
                <li>Real-time feature computation kills more ML projects than model accuracy - precompute everything you can</li>
                <li>The best model is the one that ships - I've seen too many teams optimize F1 scores while competitors capture markets</li>
            </ul>
        </section>

        <section class="tech-stack">
            <h2>// Tech Stack</h2>
            <div class="tech-grid">
                <span class="tech-item">PyTorch</span>
                <span class="tech-item">Ray Serve</span>
                <span class="tech-item">Redis</span>
                <span class="tech-item">Kubernetes</span>
                <span class="tech-item">Apache Beam</span>
                <span class="tech-item">ONNX Runtime</span>
                <span class="tech-item">Prometheus</span>
                <span class="tech-item">FastAPI</span>
            </div>
        </section>

        <footer>
            <p class="terminal-cursor">$ echo "The best model is the one that ships" <span class="cursor">_</span></p>
        </footer>
    </div>
</body>
</html>