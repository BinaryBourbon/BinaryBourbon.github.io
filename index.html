<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BinaryBourbon - AI Engineer</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <h1 class="glitch" data-text="BinaryBourbon">BinaryBourbon</h1>
            <p class="tagline">AI Engineer | Neural Network Whisperer | Gradient Descent Enthusiast</p>
        </header>

        <section class="bio">
            <h2>// About</h2>
            <p>Building intelligence one tensor at a time. Specializing in transformer architectures, 
            distributed training, and making machines think they're thinking.</p>
            
            <div class="stats">
                <div class="stat">
                    <span class="number">1.76T</span>
                    <span class="label">Parameters Trained</span>
                </div>
                <div class="stat">
                    <span class="number">∞</span>
                    <span class="label">Learning Rate</span>
                </div>
                <div class="stat">
                    <span class="number">0</span>
                    <span class="label">Hallucinations</span>
                </div>
            </div>
        </section>

        <section class="facts">
            <h2>// AI Engineering Facts</h2>
            <ul class="fact-list">
                <li>The Transformer architecture (2017) revolutionized NLP by using self-attention mechanisms, enabling models to process sequences in parallel rather than sequentially</li>
                <li>GPT-4 has approximately 1.76 trillion parameters, requiring over 25,000 A100 GPUs and costing ~$100M to train</li>
                <li>Mixture of Experts (MoE) models like Mixtral activate only ~13% of parameters per token, achieving GPT-3.5 performance with 5x fewer active parameters</li>
                <li>FlashAttention reduces memory usage from O(N²) to O(N) by chunking attention computation, enabling 10x longer context windows</li>
                <li>Constitutional AI uses RLHF with AI-generated critiques, reducing human annotation needs by 90% while improving alignment</li>
                <li>Quantization techniques like QLoRA enable fine-tuning 65B parameter models on a single 48GB GPU by using 4-bit precision</li>
                <li>Chain-of-Thought prompting improves reasoning accuracy by up to 40% on complex tasks by forcing step-by-step problem decomposition</li>
            </ul>
        </section>

        <section class="tech-stack">
            <h2>// Tech Stack</h2>
            <div class="tech-grid">
                <span class="tech-item">PyTorch</span>
                <span class="tech-item">JAX</span>
                <span class="tech-item">Transformers</span>
                <span class="tech-item">CUDA</span>
                <span class="tech-item">DeepSpeed</span>
                <span class="tech-item">Triton</span>
                <span class="tech-item">LangChain</span>
                <span class="tech-item">vLLM</span>
            </div>
        </section>

        <footer>
            <p class="terminal-cursor">$ echo "Optimizing the future, one epoch at a time" <span class="cursor">_</span></p>
        </footer>
    </div>
</body>
</html>